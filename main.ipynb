{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:45:53.066787Z",
     "iopub.status.busy": "2025-04-11T06:45:53.066526Z",
     "iopub.status.idle": "2025-04-11T06:45:53.554019Z",
     "shell.execute_reply": "2025-04-11T06:45:53.553441Z",
     "shell.execute_reply.started": "2025-04-11T06:45:53.066763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from Transformer import Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:45:53.555125Z",
     "iopub.status.busy": "2025-04-11T06:45:53.554780Z",
     "iopub.status.idle": "2025-04-11T06:45:55.796564Z",
     "shell.execute_reply": "2025-04-11T06:45:55.795989Z",
     "shell.execute_reply.started": "2025-04-11T06:45:53.555099Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizers from Hugging Face\n",
    "tokenizer_trg = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer_src = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Ensure that each tokenizer has the special tokens you need. Your transformer code uses:\n",
    "#   - A padding token ('<pad>')\n",
    "#   - A start token ('<sos>')\n",
    "#   - An end token ('<eos>')\n",
    "# If these are not already defined, add them.\n",
    "for t in [tokenizer_src, tokenizer_trg]:\n",
    "    special_tokens = {}\n",
    "    if t.pad_token is None:\n",
    "        special_tokens['pad_token'] = '<pad>'\n",
    "    if t.bos_token is None:\n",
    "        special_tokens['bos_token'] = '<sos>'\n",
    "    if t.eos_token is None:\n",
    "        special_tokens['eos_token'] = '<eos>'\n",
    "    if special_tokens:\n",
    "        t.add_special_tokens(special_tokens)\n",
    "\n",
    "# Now “wrap” the tokenizers into an object that provides the expected attributes.\n",
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        # Extract vocab as a dictionary: word -> index.\n",
    "        self.vocab = type('', (), {})()  # create a dummy object\n",
    "        self.vocab.stoi = tokenizer.get_vocab()\n",
    "        # Build the reverse mapping: index -> word.\n",
    "        self.vocab.itos = {idx: tok for tok, idx in self.vocab.stoi.items()}\n",
    "    \n",
    "    # A simple preprocessing function that converts text into tokens.\n",
    "    # You can customize this based on your needs.\n",
    "    def preprocess(self, text):\n",
    "        # Here we use the tokenizer's built-in basic tokenization.\n",
    "        # In many cases (especially with BERT), you might want to use lower case,\n",
    "        # strip extra spaces, etc.\n",
    "        return self.tokenizer.tokenize(text)\n",
    "\n",
    "# Create wrappers\n",
    "SRC = TokenizerWrapper(tokenizer_src)\n",
    "TRG = TokenizerWrapper(tokenizer_trg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:45:55.797731Z",
     "iopub.status.busy": "2025-04-11T06:45:55.797339Z",
     "iopub.status.idle": "2025-04-11T06:45:55.806706Z",
     "shell.execute_reply": "2025-04-11T06:45:55.805953Z",
     "shell.execute_reply.started": "2025-04-11T06:45:55.797681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ParallelENVIDataset(Dataset):\n",
    "    def __init__(self, src_file, trg_file, src_wrapper, trg_wrapper, max_src_length=160, max_trg_length=160):\n",
    "        \"\"\"\n",
    "        ...\n",
    "        max_src_length and max_trg_length: Maximum allowed number of tokens for source and target sequences\n",
    "        \"\"\"\n",
    "        with open(src_file, 'r', encoding='utf-8') as f:\n",
    "            self.src_sentences = [line.strip() for line in f]\n",
    "        with open(trg_file, 'r', encoding='utf-8') as f:\n",
    "            self.trg_sentences = [line.strip() for line in f]\n",
    "        \n",
    "        self.src_wrapper = src_wrapper\n",
    "        self.trg_wrapper = trg_wrapper\n",
    "        self.max_src_length = max_src_length\n",
    "        self.max_trg_length = max_trg_length\n",
    "        \n",
    "        assert len(self.src_sentences) == len(self.trg_sentences), \\\n",
    "            \"Source and target files must have the same number of lines!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a tuple (source_idx_list, target_idx_list).\"\"\"\n",
    "        src_text = self.src_sentences[idx]\n",
    "        trg_text = self.trg_sentences[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        src_tokens = self.src_wrapper.preprocess(src_text)\n",
    "        trg_tokens = self.trg_wrapper.preprocess(trg_text)\n",
    "        \n",
    "        # Optionally, truncate tokens if length exceeds maximum allowed\n",
    "        if self.max_src_length:\n",
    "            src_tokens = src_tokens[:self.max_src_length]\n",
    "        if self.max_trg_length:\n",
    "            trg_tokens = trg_tokens[:self.max_trg_length]\n",
    "        \n",
    "        # Retrieve pad token string from tokenizer\n",
    "        src_pad_token = self.src_wrapper.tokenizer.pad_token  # e.g., \"[PAD]\"\n",
    "        trg_pad_token = self.trg_wrapper.tokenizer.pad_token  # e.g., \"[PAD]\"\n",
    "        \n",
    "        # Convert tokens to indices, defaulting missing tokens to the pad token id.\n",
    "        src_indices = [\n",
    "            self.src_wrapper.vocab.stoi.get(t, self.src_wrapper.vocab.stoi[src_pad_token])\n",
    "            for t in src_tokens\n",
    "        ]\n",
    "        trg_indices = [\n",
    "            self.trg_wrapper.vocab.stoi.get(t, self.trg_wrapper.vocab.stoi[trg_pad_token])\n",
    "            for t in trg_tokens\n",
    "        ]\n",
    "        \n",
    "        # Optionally, add <sos> and <eos> tokens around the target sequence if needed.\n",
    "        # For example:\n",
    "        sos_token = self.trg_wrapper.tokenizer.bos_token  # e.g., \"<sos>\"\n",
    "        eos_token = self.trg_wrapper.tokenizer.eos_token  # e.g., \"<eos>\"\n",
    "        trg_indices = ([self.trg_wrapper.vocab.stoi[sos_token]] + trg_indices + [self.trg_wrapper.vocab.stoi[eos_token]])\n",
    "        \n",
    "        return torch.LongTensor(src_indices), torch.LongTensor(trg_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:45:55.807738Z",
     "iopub.status.busy": "2025-04-11T06:45:55.807470Z",
     "iopub.status.idle": "2025-04-11T06:45:55.823574Z",
     "shell.execute_reply": "2025-04-11T06:45:55.823073Z",
     "shell.execute_reply.started": "2025-04-11T06:45:55.807712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (src_indices, trg_indices) from ParallelENVIDataset.\n",
    "    We'll pad them to have uniform lengths within the batch.\n",
    "    \"\"\"\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    \n",
    "    # Convert tuple of Tensors -> list of lengths.\n",
    "    src_lens = [len(s) for s in src_batch]\n",
    "    trg_lens = [len(t) for t in trg_batch]\n",
    "    \n",
    "    max_src_len = max(src_lens)\n",
    "    max_trg_len = max(trg_lens)\n",
    "    \n",
    "    # Get the pad token strings from your tokenizer wrappers.\n",
    "    src_pad_str = SRC.tokenizer.pad_token  # likely \"[PAD]\"\n",
    "    trg_pad_str = TRG.tokenizer.pad_token  # likely \"[PAD]\"\n",
    "    \n",
    "    # Now look up the correct ID from the vocabulary mapping.\n",
    "    src_pad_id = SRC.vocab.stoi[src_pad_str]\n",
    "    trg_pad_id = TRG.vocab.stoi[trg_pad_str]\n",
    "    \n",
    "    # Pad the sequences.\n",
    "    padded_src = []\n",
    "    padded_trg = []\n",
    "    for s, t in zip(src_batch, trg_batch):\n",
    "        # s: pad to max_src_len\n",
    "        pad_amount = max_src_len - len(s)\n",
    "        padded_s = torch.cat([s, s.new_full((pad_amount,), src_pad_id)])\n",
    "        padded_src.append(padded_s)\n",
    "        \n",
    "        # t: pad to max_trg_len\n",
    "        pad_amount = max_trg_len - len(t)\n",
    "        padded_t = torch.cat([t, t.new_full((pad_amount,), trg_pad_id)])\n",
    "        padded_trg.append(padded_t)\n",
    "        \n",
    "    # Stack the padded sequences into a single tensor.\n",
    "    padded_src = torch.stack(padded_src, dim=0)\n",
    "    padded_trg = torch.stack(padded_trg, dim=0)\n",
    "    \n",
    "    return padded_src, padded_trg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:45:55.824384Z",
     "iopub.status.busy": "2025-04-11T06:45:55.824193Z",
     "iopub.status.idle": "2025-04-11T06:45:56.025944Z",
     "shell.execute_reply": "2025-04-11T06:45:56.025144Z",
     "shell.execute_reply.started": "2025-04-11T06:45:55.824368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_en = \"train.en\"\n",
    "train_vi = \"train.vi\"\n",
    "\n",
    "train_dataset = ParallelENVIDataset(\n",
    "    src_file=train_en,\n",
    "    trg_file=train_vi,\n",
    "    src_wrapper=SRC,\n",
    "    trg_wrapper=TRG\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=30,       # pick a suitable batch size\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:45:56.028078Z",
     "iopub.status.busy": "2025-04-11T06:45:56.027850Z",
     "iopub.status.idle": "2025-04-11T06:45:57.420384Z",
     "shell.execute_reply": "2025-04-11T06:45:57.419602Z",
     "shell.execute_reply.started": "2025-04-11T06:45:56.028061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "src_vocab_size = len(SRC.vocab.stoi)\n",
    "trg_vocab_size = len(TRG.vocab.stoi)\n",
    "src_pad_idx = SRC.tokenizer.pad_token_id\n",
    "trg_pad_idx = TRG.tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    trg_vocab_size=trg_vocab_size,\n",
    "    src_pad_idx=src_pad_idx,\n",
    "    trg_pad_idx=trg_pad_idx,\n",
    "    embed_size=512,        # or your chosen dimension\n",
    "    num_layers=6,          # or however many you want\n",
    "    forward_expansion=4,\n",
    "    heads=8,\n",
    "    dropout_probability=0.1,\n",
    "    device=device,\n",
    "    position_max_length=200 # set max length as needed\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:45:57.421296Z",
     "iopub.status.busy": "2025-04-11T06:45:57.421074Z",
     "iopub.status.idle": "2025-04-11T06:45:58.462788Z",
     "shell.execute_reply": "2025-04-11T06:45:58.461994Z",
     "shell.execute_reply.started": "2025-04-11T06:45:57.421279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Using the Scheduled Optimizer instead of the default Adam\n",
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, init_lr, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.init_lr = init_lr\n",
    "        self.d_model = d_model\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_steps = 0\n",
    "\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients with the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        d_model = self.d_model\n",
    "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
    "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
    "\n",
    "    def state_dict(self):\n",
    "        optimizer_state_dict = {\n",
    "            'init_lr':self.init_lr,\n",
    "            'd_model':self.d_model,\n",
    "            'n_warmup_steps':self.n_warmup_steps,\n",
    "            'n_steps':self.n_steps,\n",
    "            '_optimizer':self._optimizer.state_dict(),\n",
    "        }\n",
    "        \n",
    "        return optimizer_state_dict\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.init_lr = state_dict['init_lr']\n",
    "        self.d_model = state_dict['d_model']\n",
    "        self.n_warmup_steps = state_dict['n_warmup_steps']\n",
    "        self.n_steps = state_dict['n_steps']\n",
    "        \n",
    "        self._optimizer.load_state_dict(state_dict['_optimizer'])\n",
    "        \n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "            \n",
    "adam = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "optimizer = ScheduledOptim(adam, init_lr=0.2, d_model=512, n_warmup_steps=4000)\n",
    "# Cross Entropy Loss ignoring the padding index\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:45:58.464051Z",
     "iopub.status.busy": "2025-04-11T06:45:58.463640Z",
     "iopub.status.idle": "2025-04-11T09:43:45.375904Z",
     "shell.execute_reply": "2025-04-11T09:43:45.374867Z",
     "shell.execute_reply.started": "2025-04-11T06:45:58.464029Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [0/4444], Loss: 11.2357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (757 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1000/4444], Loss: 6.1666\n",
      "Epoch [1/10], Step [2000/4444], Loss: 5.7099\n",
      "Epoch [1/10], Step [3000/4444], Loss: 5.1315\n",
      "Epoch [1/10], Step [4000/4444], Loss: 4.7252\n",
      "*** Epoch [1] Completed; Avg Loss: 5.8618\n",
      "Model saved with avg loss 5.8618 at epoch 1\n",
      "Epoch [2/10], Step [0/4444], Loss: 4.5423\n",
      "Epoch [2/10], Step [1000/4444], Loss: 4.6055\n",
      "Epoch [2/10], Step [2000/4444], Loss: 4.2364\n",
      "Epoch [2/10], Step [3000/4444], Loss: 3.9192\n",
      "Epoch [2/10], Step [4000/4444], Loss: 4.3977\n",
      "*** Epoch [2] Completed; Avg Loss: 4.2651\n",
      "Model saved with avg loss 4.2651 at epoch 2\n",
      "Epoch [3/10], Step [0/4444], Loss: 3.6234\n",
      "Epoch [3/10], Step [1000/4444], Loss: 3.7069\n",
      "Epoch [3/10], Step [2000/4444], Loss: 3.5824\n",
      "Epoch [3/10], Step [3000/4444], Loss: 3.5833\n",
      "Epoch [3/10], Step [4000/4444], Loss: 3.7638\n",
      "*** Epoch [3] Completed; Avg Loss: 3.7958\n",
      "Model saved with avg loss 3.7958 at epoch 3\n",
      "Epoch [4/10], Step [0/4444], Loss: 3.7781\n",
      "Epoch [4/10], Step [1000/4444], Loss: 3.5151\n",
      "Epoch [4/10], Step [2000/4444], Loss: 3.5620\n",
      "Epoch [4/10], Step [3000/4444], Loss: 3.5115\n",
      "Epoch [4/10], Step [4000/4444], Loss: 3.4845\n",
      "*** Epoch [4] Completed; Avg Loss: 3.5241\n",
      "Model saved with avg loss 3.5241 at epoch 4\n",
      "Epoch [5/10], Step [0/4444], Loss: 3.5708\n",
      "Epoch [5/10], Step [1000/4444], Loss: 3.2726\n",
      "Epoch [5/10], Step [2000/4444], Loss: 3.2553\n",
      "Epoch [5/10], Step [3000/4444], Loss: 3.3646\n",
      "Epoch [5/10], Step [4000/4444], Loss: 3.3510\n",
      "*** Epoch [5] Completed; Avg Loss: 3.3341\n",
      "Model saved with avg loss 3.3341 at epoch 5\n",
      "Epoch [6/10], Step [0/4444], Loss: 3.4058\n",
      "Epoch [6/10], Step [1000/4444], Loss: 3.2401\n",
      "Epoch [6/10], Step [2000/4444], Loss: 2.9446\n",
      "Epoch [6/10], Step [3000/4444], Loss: 3.5485\n",
      "Epoch [6/10], Step [4000/4444], Loss: 3.4116\n",
      "*** Epoch [6] Completed; Avg Loss: 3.1885\n",
      "Model saved with avg loss 3.1885 at epoch 6\n",
      "Epoch [7/10], Step [0/4444], Loss: 2.8741\n",
      "Epoch [7/10], Step [1000/4444], Loss: 2.8551\n",
      "Epoch [7/10], Step [2000/4444], Loss: 3.1751\n",
      "Epoch [7/10], Step [3000/4444], Loss: 3.0555\n",
      "Epoch [7/10], Step [4000/4444], Loss: 3.1029\n",
      "*** Epoch [7] Completed; Avg Loss: 3.0727\n",
      "Model saved with avg loss 3.0727 at epoch 7\n",
      "Epoch [8/10], Step [0/4444], Loss: 2.5801\n",
      "Epoch [8/10], Step [1000/4444], Loss: 2.7637\n",
      "Epoch [8/10], Step [2000/4444], Loss: 2.9555\n",
      "Epoch [8/10], Step [3000/4444], Loss: 3.0060\n",
      "Epoch [8/10], Step [4000/4444], Loss: 3.0271\n",
      "*** Epoch [8] Completed; Avg Loss: 2.9774\n",
      "Model saved with avg loss 2.9774 at epoch 8\n",
      "Epoch [9/10], Step [0/4444], Loss: 2.9688\n",
      "Epoch [9/10], Step [1000/4444], Loss: 2.7904\n",
      "Epoch [9/10], Step [2000/4444], Loss: 3.0126\n",
      "Epoch [9/10], Step [3000/4444], Loss: 2.8991\n",
      "Epoch [9/10], Step [4000/4444], Loss: 3.2064\n",
      "*** Epoch [9] Completed; Avg Loss: 2.8955\n",
      "Model saved with avg loss 2.8955 at epoch 9\n",
      "Epoch [10/10], Step [0/4444], Loss: 2.8475\n",
      "Epoch [10/10], Step [1000/4444], Loss: 2.9391\n",
      "Epoch [10/10], Step [2000/4444], Loss: 2.8678\n",
      "Epoch [10/10], Step [3000/4444], Loss: 2.8221\n",
      "Epoch [10/10], Step [4000/4444], Loss: 3.1221\n",
      "*** Epoch [10] Completed; Avg Loss: 2.8251\n",
      "Model saved with avg loss 2.8251 at epoch 10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_epochs = 10  # or whichever number of epochs you choose\n",
    "best_loss = float(\"inf\")  # Initialize best loss to a high value\n",
    "checkpoint_path = \"best_model.pth\"  # Filepath to save the best model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # set model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (src_batch, trg_batch) in enumerate(train_loader):\n",
    "        src_batch = src_batch.to(device)\n",
    "        trg_batch = trg_batch.to(device)\n",
    "        \n",
    "        # SHIFT target for teacher forcing:\n",
    "        # Input to the decoder: trg_batch[:, :-1]\n",
    "        # Expected output: trg_batch[:, 1:]\n",
    "        outputs = model(src_batch, trg_batch[:, :-1])\n",
    "        \n",
    "        # outputs: [batch_size, trg_len - 1, vocab_size]\n",
    "        # Reshape to [batch_size * (trg_len - 1), vocab_size]\n",
    "        outputs = outputs.reshape(-1, outputs.shape[2])\n",
    "        # ground_truth: [batch_size, trg_len - 1] -> reshape to [batch_size * (trg_len - 1)]\n",
    "        ground_truth = trg_batch[:, 1:].reshape(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, ground_truth)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step_and_update_lr()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 1000 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"*** Epoch [{epoch+1}] Completed; Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save the model if the average loss for this epoch is lower than the best loss so far.\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Model saved with avg loss {avg_loss:.4f} at epoch {epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:43:45.377302Z",
     "iopub.status.busy": "2025-04-11T09:43:45.377003Z",
     "iopub.status.idle": "2025-04-11T09:43:45.383721Z",
     "shell.execute_reply": "2025-04-11T09:43:45.382822Z",
     "shell.execute_reply.started": "2025-04-11T09:43:45.377275Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='best_model.pth' target='_blank'>best_model.pth</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/best_model.pth"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink(r'best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use can load my model from the link I provided in README\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "model.eval()  # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:49:42.685135Z",
     "iopub.status.busy": "2025-04-11T09:49:42.684325Z",
     "iopub.status.idle": "2025-04-11T09:49:43.183482Z",
     "shell.execute_reply": "2025-04-11T09:49:43.182736Z",
     "shell.execute_reply.started": "2025-04-11T09:49:42.685108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dang\\AppData\\Local\\Temp\\ipykernel_20380\\420263240.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 5. Define or import your translation functions:\n",
    "def nopeak_mask(size, device):\n",
    "    np_mask = torch.triu(torch.ones((1, size, size), device=device), diagonal=1).bool()\n",
    "    return np_mask == 0\n",
    "\n",
    "def greedy_decode(src, model, SRC, TRG, device, max_len):\n",
    "    \"\"\"\n",
    "    Greedy decoding for generating translation.\n",
    "    \"\"\"\n",
    "    # Create source mask\n",
    "    src_mask = (src != SRC.vocab.stoi[SRC.tokenizer.pad_token]).unsqueeze(-2)\n",
    "    encoder_output = model.encoder(src, src_mask)\n",
    "\n",
    "    # Use the target tokenizer's bos_token_id if available, otherwise fall back to TRG.vocab.stoi\n",
    "    init_tok = TRG.tokenizer.bos_token_id if TRG.tokenizer.bos_token_id is not None else TRG.vocab.stoi.get('<sos>')\n",
    "    if init_tok is None:\n",
    "        raise ValueError(\"No valid start token found in target tokenizer/vocab.\")\n",
    "\n",
    "    outputs = torch.LongTensor([[init_tok]]).to(device)\n",
    "\n",
    "    for i in range(1, max_len):\n",
    "        trg_mask = nopeak_mask(outputs.size(1), device)\n",
    "        # Directly get the output from the decoder. This output is already the final logits.\n",
    "        out = model.decoder(outputs, encoder_output, src_mask, trg_mask)\n",
    "        out = F.softmax(out, dim=-1)\n",
    "\n",
    "        # Choose the token with the highest probability for the last time step.\n",
    "        next_token = torch.argmax(out[:, -1, :], dim=-1).unsqueeze(0)\n",
    "        outputs = torch.cat([outputs, next_token], dim=1)\n",
    "\n",
    "        # Retrieve end token similarly.\n",
    "        eos_tok = TRG.tokenizer.eos_token_id if TRG.tokenizer.eos_token_id is not None else TRG.vocab.stoi.get('<eos>')\n",
    "        if eos_tok is None:\n",
    "            raise ValueError(\"No valid end token found in target tokenizer/vocab.\")\n",
    "\n",
    "        if next_token.item() == eos_tok:\n",
    "            break\n",
    "\n",
    "    token_ids = outputs.squeeze().tolist()\n",
    "    if token_ids[-1] == eos_tok:\n",
    "        token_ids = token_ids[1:-1]\n",
    "    else:\n",
    "        token_ids = token_ids[1:]\n",
    "    \n",
    "    translation = ' '.join([TRG.vocab.itos[tok] for tok in token_ids])\n",
    "    return translation\n",
    "def translate_sentence(sentence, model, SRC, TRG, device, max_len):\n",
    "    model.eval()\n",
    "    tokens = SRC.preprocess(sentence)\n",
    "    indexed = [SRC.vocab.stoi.get(tok, SRC.vocab.stoi[tokenizer_src.pad_token]) for tok in tokens]\n",
    "    src_tensor = torch.LongTensor([indexed]).to(device)\n",
    "    translation = greedy_decode(src_tensor, model, SRC, TRG, device, max_len)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:53:51.029605Z",
     "iopub.status.busy": "2025-04-11T09:53:51.029099Z",
     "iopub.status.idle": "2025-04-11T09:53:51.338937Z",
     "shell.execute_reply": "2025-04-11T09:53:51.338305Z",
     "shell.execute_reply.started": "2025-04-11T09:53:51.029584Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: Tôi thực sự thích phần của thử thách thậm chí sau khi thành công . Nó làm tôi cảm thấy gì đó trong tôi .\n"
     ]
    }
   ],
   "source": [
    "max_len = 50 # Maximum allowed target sequence length for decoding\n",
    "source_sentence = \"I really liked the part about challenging yourself even after achieving success. It shifted something in me.\"  # Sample English sentence\n",
    "\n",
    "# Call the translate_sentence function:\n",
    "translation = translate_sentence(source_sentence, model, SRC, TRG, device, max_len)\n",
    "print(\"Translation:\", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7106408,
     "sourceId": 11355514,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
